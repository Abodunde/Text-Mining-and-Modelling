{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language modelling with LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will be using LSTMs for language modelling by learning sequences of sentences (windows) that lead up to a certain word.\n",
    "\n",
    "Note:\n",
    "first time running, you will need to install ``tensorflow`` and ``keras``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Input\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\abodunde.ojo_kuda\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Unzipping corpora\\gutenberg.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Emma by Jane Austen from the Gutenberg corpus again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "emma_sents = nltk.corpus.gutenberg.sents('austen-emma.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']'], ['VOLUME', 'I'], ['CHAPTER', 'I'], ['Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', ',', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', ';', 'and', 'had', 'lived', 'nearly', 'twenty', '-', 'one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her', '.'], ['She', 'was', 'the', 'youngest', 'of', 'the', 'two', 'daughters', 'of', 'a', 'most', 'affectionate', ',', 'indulgent', 'father', ';', 'and', 'had', ',', 'in', 'consequence', 'of', 'her', 'sister', \"'\", 's', 'marriage', ',', 'been', 'mistress', 'of', 'his', 'house', 'from', 'a', 'very', 'early', 'period', '.']]\n"
     ]
    }
   ],
   "source": [
    "# print the first 5 sentences\n",
    "print(emma_sents[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we store all the words as integers (like a dictionary, similar to the one-hot vector):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']']\n",
      "[1, 2, 3, 4, 5, 6, 7]\n",
      "['VOLUME', 'I']\n",
      "[8, 9]\n",
      "['CHAPTER', 'I']\n",
      "[10, 9]\n",
      "['Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', ',', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', ';', 'and', 'had', 'lived', 'nearly', 'twenty', '-', 'one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her', '.']\n",
      "[2, 11, 12, 13, 12, 14, 12, 15, 16, 12, 17, 18, 19, 20, 15, 21, 22, 12, 23, 24, 25, 26, 27, 28, 29, 30, 27, 31, 32, 15, 33, 34, 35, 36, 37, 38, 39, 40, 28, 41, 17, 42, 43, 24, 44, 45, 46, 47, 48]\n",
      "['She', 'was', 'the', 'youngest', 'of', 'the', 'two', 'daughters', 'of', 'a', 'most', 'affectionate', ',', 'indulgent', 'father', ';', 'and', 'had', ',', 'in', 'consequence', 'of', 'her', 'sister', \"'\", 's', 'marriage', ',', 'been', 'mistress', 'of', 'his', 'house', 'from', 'a', 'very', 'early', 'period', '.']\n",
      "[49, 50, 28, 51, 27, 28, 52, 53, 27, 18, 54, 55, 12, 56, 57, 32, 15, 33, 12, 40, 58, 27, 47, 59, 60, 61, 62, 12, 63, 64, 27, 65, 66, 67, 18, 42, 68, 69, 48]\n"
     ]
    }
   ],
   "source": [
    "# stores the words -> integers\n",
    "word_dict = {}\n",
    "# stores the integers -> words\n",
    "reverse_dict = {}\n",
    "\n",
    "# keeps track of the words\n",
    "no_words = 1\n",
    "\n",
    "int_sents = []\n",
    "longest = 0\n",
    "for i, sentence in enumerate(emma_sents[:1000]):\n",
    "    ints = []\n",
    "    \n",
    "    for word in sentence:\n",
    "        if word not in word_dict.keys():\n",
    "            word_dict[word] = no_words\n",
    "            reverse_dict[no_words] = word\n",
    "            no_words += 1\n",
    "        # add word to the integer list\n",
    "        ints.append(word_dict[word])\n",
    "    \n",
    "    # Print to illustrate the conversion\n",
    "    if i<5:\n",
    "        print(sentence)\n",
    "        print(ints)\n",
    "    # Keep track of the longest sentence\n",
    "    if len(sentence) > longest:\n",
    "        longest = len(sentence)\n",
    "        \n",
    "    # store the integer sentence\n",
    "    int_sents.append(ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Words:  2728\n",
      "#Sentences:  1000\n",
      "Length:  172\n"
     ]
    }
   ],
   "source": [
    "print('#Words: ',len(word_dict))\n",
    "print('#Sentences: ',len(int_sents))\n",
    "print('Length: ', longest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we have to pad our sequences that are not long enough, adding 0's at the begining of each ``int_sents`` (although not as necessary in this case):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_sents = sequence.pad_sequences(int_sents, maxlen=longest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "       10,  9])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an example\n",
    "int_sents[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider each input vector contains 30 words, and the corresponding output is the 31st word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input is [1, 2, 3]\n",
      "output is 4\n"
     ]
    }
   ],
   "source": [
    "tt = [0,1,2,3,4,5,6]\n",
    "print('input is', tt[1:1+3])\n",
    "print('output is',tt[1+3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "window = 30\n",
    "\n",
    "# Go through the sentences and store the window (sequence of words) as X\n",
    "# y contains the word at the end of the window\n",
    "for sent in int_sents:\n",
    "    for i in range(0, len(sent) - window, 1):\n",
    "        window_x = sent[i:i + window]\n",
    "        window_y = sent[i + window]\n",
    "        X.append(window_x)\n",
    "        y.append(window_y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input is [  0   0   0   0   0   0   0 145 166  12 167  12  50 168 169 170 171  12\n",
      " 172 129 173 174   3 121 175 176  88 177  17  47]\n",
      "output is 48\n"
     ]
    }
   ],
   "source": [
    "# For example\n",
    "sent = int_sents[10]\n",
    "i= len(sent) - window -1 #since range() creates until len(sent) - window -1\n",
    "print('input is', sent[i:i + window])\n",
    "print('output is',sent[i + window])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to store our data in the correct shape (#sentences x length window x #features (1 - the words)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(142000, 30, 1)\n",
      "(142000, 2729)\n"
     ]
    }
   ],
   "source": [
    "X = np.reshape(X, (len(X), window, 1))\n",
    "\n",
    "# For the y-value, we use one-hot encoding\n",
    "y = to_categorical(y)    \n",
    "\n",
    "print(np.shape(X))\n",
    "print(np.shape(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [681]\n",
      " [578]\n",
      " [157]\n",
      " [332]\n",
      " [ 27]\n",
      " [136]\n",
      " [682]\n",
      " [ 27]\n",
      " [680]\n",
      " [490]\n",
      " [334]\n",
      " [102]\n",
      " [254]\n",
      " [683]\n",
      " [ 12]\n",
      " [  9]\n",
      " [ 77]\n",
      " [ 63]\n",
      " [ 40]\n",
      " [209]\n",
      " [684]\n",
      " [ 17]\n",
      " [501]\n",
      " [685]\n",
      " [ 32]\n",
      " [103]]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# For example, although many vectors are 0's since we only consider a dictionary of size 30\n",
    "ind = 12345\n",
    "print(X[ind])\n",
    "print(y[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[:2000]\n",
    "y = y[:2000]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(660, 2729)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model:\n",
    "\n",
    "Note:\n",
    "\n",
    "- check ``keras.LSTM`` configurations here https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM\n",
    "\n",
    "- ``Dense`` implements the operation: ``output = activation(dot(input, kernel) + bias)``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7/7 [==============================] - 4s 204ms/step - loss: 7.8908 - accuracy: 0.8069 - mae: 7.3259e-04 - val_loss: 7.8224 - val_accuracy: 0.8134 - val_mae: 7.3257e-04\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 7.6436 - accuracy: 0.8125 - mae: 7.3251e-04 - val_loss: 6.7601 - val_accuracy: 0.8097 - val_mae: 7.3196e-04\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 5.8210 - accuracy: 0.8162 - mae: 7.2977e-04 - val_loss: 4.7474 - val_accuracy: 0.8097 - val_mae: 7.2283e-04\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 3.8873 - accuracy: 0.8116 - mae: 7.0125e-04 - val_loss: 2.7850 - val_accuracy: 0.7948 - val_mae: 6.1855e-04\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 2.0664 - accuracy: 0.8116 - mae: 4.8960e-04 - val_loss: 1.7180 - val_accuracy: 0.7948 - val_mae: 3.0300e-04\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 1.3787 - accuracy: 0.8125 - mae: 2.4224e-04 - val_loss: 1.5224 - val_accuracy: 0.7948 - val_mae: 1.9276e-04\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 1.2202 - accuracy: 0.8172 - mae: 1.7495e-04 - val_loss: 1.4674 - val_accuracy: 0.7948 - val_mae: 1.7146e-04\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 1.1417 - accuracy: 0.8162 - mae: 1.6018e-04 - val_loss: 1.4332 - val_accuracy: 0.7948 - val_mae: 1.6651e-04\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 1.0725 - accuracy: 0.8125 - mae: 1.5641e-04 - val_loss: 1.4129 - val_accuracy: 0.7948 - val_mae: 1.6528e-04\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 1.0306 - accuracy: 0.8144 - mae: 1.5574e-04 - val_loss: 1.4056 - val_accuracy: 0.7948 - val_mae: 1.6592e-04\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.9915 - accuracy: 0.8162 - mae: 1.5638e-04 - val_loss: 1.4074 - val_accuracy: 0.7985 - val_mae: 1.6642e-04\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.9657 - accuracy: 0.8172 - mae: 1.5703e-04 - val_loss: 1.4122 - val_accuracy: 0.7948 - val_mae: 1.6661e-04\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.9422 - accuracy: 0.8190 - mae: 1.5680e-04 - val_loss: 1.4174 - val_accuracy: 0.7948 - val_mae: 1.6689e-04\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 0.9248 - accuracy: 0.8190 - mae: 1.5719e-04 - val_loss: 1.4246 - val_accuracy: 0.7948 - val_mae: 1.6691e-04\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 0s 67ms/step - loss: 0.9155 - accuracy: 0.8144 - mae: 1.5673e-04 - val_loss: 1.4324 - val_accuracy: 0.7985 - val_mae: 1.6678e-04\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 1s 78ms/step - loss: 0.9043 - accuracy: 0.8181 - mae: 1.5632e-04 - val_loss: 1.4392 - val_accuracy: 0.7985 - val_mae: 1.6676e-04\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 1s 79ms/step - loss: 0.8892 - accuracy: 0.8162 - mae: 1.5595e-04 - val_loss: 1.4466 - val_accuracy: 0.7985 - val_mae: 1.6651e-04\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 0.8748 - accuracy: 0.8172 - mae: 1.5521e-04 - val_loss: 1.4535 - val_accuracy: 0.7948 - val_mae: 1.6568e-04\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 0.8617 - accuracy: 0.8209 - mae: 1.5438e-04 - val_loss: 1.4603 - val_accuracy: 0.7948 - val_mae: 1.6523e-04\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 0s 66ms/step - loss: 0.8574 - accuracy: 0.8172 - mae: 1.5421e-04 - val_loss: 1.4705 - val_accuracy: 0.7948 - val_mae: 1.6589e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x229679d84d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "no_dim = 128 # # of blocks, related to the training time\n",
    "\n",
    "model = Sequential()\n",
    "# Input is: a window of 1 feature: an integer representing the word\n",
    "model.add(LSTM(no_dim, input_shape=(window, 1))) \n",
    "model.add(Dropout(0.2))\n",
    "# The output layer predicts the word, one-hot encoded (i.e. the vector is as long as the number of words)\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy','mae'])\n",
    "model.fit(X_train, y_train, validation_split=0.2, batch_size=longest, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 128)               66560     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2729)              352041    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 418601 (1.60 MB)\n",
      "Trainable params: 418601 (1.60 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 0s 8ms/step - loss: 1.4338 - accuracy: 0.8182 - mae: 1.5202e-04\n",
      "{'loss': 1.4337700605392456, 'accuracy': 0.8181818127632141, 'mae': 0.00015202061331365258}\n"
     ]
    }
   ],
   "source": [
    "evaluation = model.evaluate(X_test, y_test,return_dict = True)\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting the words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 490ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "\n",
      "To predict:  had been living together as friend and friend very mutually attached , and Emma doing just what she liked ; highly esteeming Miss Taylor ' s judgment , but directed \n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Prediction:  of\n",
      "Actual word:  chiefly\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "\n",
      "To predict:  She was the \n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "\n",
      "To predict:  It was on the wedding - day of this beloved friend that Emma first sat \n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "Prediction:  of\n",
      "Actual word:  in\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "\n",
      "To predict:  Sixteen years had \n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "\n",
      "To predict:  Her mother had died too long ago for her to have more than an indistinct remembrance \n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Prediction:  of\n",
      "Actual word:  of\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "\n",
      "To predict:  of governess , the mildness of her temper had hardly allowed her to impose any restraint ; and the shadow of authority being now long passed away , they had \n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Prediction:  of\n",
      "Actual word:  been\n",
      "\n",
      "To predict:  Sorrow came -- a gentle sorrow -- but not at all in the shape of \n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Prediction:  of\n",
      "Actual word:  any\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "\n",
      "To predict:  She was the youngest of the two \n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Prediction:  ,\n",
      "Actual word:  daughters\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "\n",
      "To predict:  It was on the wedding - day of this beloved friend that Emma first sat in mournful thought of any continuance \n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "Prediction:  of\n",
      "Actual word:  .\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "\n",
      "To predict:  Sixteen years had Miss Taylor been in Mr . Woodhouse ' s family , less as a governess than a friend , very fond of both \n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Prediction:  of\n",
      "Actual word:  daughters\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "\n",
      "To predict:  The real evils , indeed , of Emma ' s situation were the power of having rather too much her own way \n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Prediction:  of\n",
      "Actual word:  ,\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "\n",
      "To predict:  The danger , however , was at present so unperceived , that they did not by any \n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Prediction:  of\n",
      "Actual word:  means\n",
      "\n",
      "To predict:  Sorrow came -- a gentle sorrow -- but not at \n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Prediction:  of\n",
      "Actual word:  all\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "\n",
      "To predict:  The danger , however , was at present so unperceived , that they did not by any means \n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Prediction:  of\n",
      "Actual word:  rank\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "\n",
      "To predict:  Even before Miss Taylor had ceased to hold the nominal office of governess , the mildness of her temper had hardly allowed her to impose any restraint ; and the \n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Prediction:  of\n",
      "Actual word:  shadow\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n"
     ]
    }
   ],
   "source": [
    "no_words = 0\n",
    "for x_i, y_i in zip(X_test, y_test):\n",
    "    \n",
    "    # We need to reshape the test again\n",
    "    x = np.reshape(x_i, (1, len(x_i), 1))\n",
    "    \n",
    "    sentence = ''\n",
    "    for word in x[0]:\n",
    "        if np.sum(word) > 0:\n",
    "            sentence += reverse_dict[word[0]] + \" \"\n",
    "    if sentence != '':\n",
    "        print('\\nTo predict: ', sentence)\n",
    "    \n",
    "    prediction = model.predict(x)\n",
    "    \n",
    "    # The LSTM returns a probability for every word, we take the highest probability (argmax)\n",
    "    i_x = np.argmax(prediction)\n",
    "    i_y = np.argmax(y_i)\n",
    "    if i_x > 0:\n",
    "        print('Prediction: ', reverse_dict[i_x])\n",
    "        print('Actual word: ', reverse_dict[i_y])\n",
    "    no_words += 1\n",
    "    if no_words > 100:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
