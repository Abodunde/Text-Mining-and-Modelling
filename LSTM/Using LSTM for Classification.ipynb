{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTMs for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, LSTMs are going to be used to predict the label (e.g. sentiment) of a sequence.\n",
    "\n",
    "We are going to use `keras` to build LSTM network, using function `keras.layers.LSTM`. First, let's install the library `tensorflow` and `keras`. This may take a few seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imdb dataset: https://keras.io/api/datasets/imdb/#getwordindex-function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 2000 #We use top max_features most common words to build a vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data (and reducing its size):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17464789/17464789 [==============================] - 4s 0us/step\n",
      "1000 train sequences\n",
      "1000 test sequences\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "x_train = x_train[:1000]\n",
    "x_test = x_test[:1000]\n",
    "y_train = y_train[:1000]\n",
    "y_test = y_test[:1000]\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to give us an idea of what the sequences look like (each number represents a different word):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X-vector: [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 1920, 2, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n",
      "Label: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"X-vector: \"+str(x_train[0]))\n",
    "print(\"Label: \"+str(y_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For your curiosity, here we just show how to retrieve the dictionary mapping word indices back to words.\n",
    "For more details, see https://stackoverflow.com/questions/42821330/restore-original-text-from-keras-s-imdb-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1641221/1641221 [==============================] - 1s 1us/step\n",
      "<START> this film was just brilliant casting location scenery story direction <UNK> really <UNK> the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same <UNK> island as myself so i loved the fact there was a real <UNK> with this film the witty <UNK> throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the <UNK> <UNK> was amazing really <UNK> at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little <UNK> that played the <UNK> of <UNK> and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all <UNK> up are such a big <UNK> for the whole film but these children are amazing and should be <UNK> for what they have done don't you think the whole story was so lovely because it was true and was <UNK> life after all that was <UNK> with us all\n"
     ]
    }
   ],
   "source": [
    "INDEX_FROM=3   # word index offset, by default\n",
    "\n",
    "word_to_id = imdb.get_word_index()\n",
    "word_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNK>\"] = 2 #unknown words according to the vovabulary\n",
    "word_to_id[\"<UNUSED>\"] = 3\n",
    "\n",
    "id_to_word = {value:key for key,value in word_to_id.items()}\n",
    "print(' '.join(id_to_word[id] for id in x_train[0] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since sequences (in this case sentences) can have different lengths, we need to make sure that they are padded: we add zeros to the beginning of the sequences that are shorter than the longest sequence so we can still train them step-by-step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform sequences\n",
      "x_train shape: (1000, 80)\n",
      "x_test shape: (1000, 80)\n"
     ]
    }
   ],
   "source": [
    "# make sure sequences have same length\n",
    "maxlen = 80  # in each sentence, cut texts  before this number of words\n",
    "\n",
    "print('Transform sequences')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X-vector: [  15  256    4    2    7    2    5  723   36   71   43  530  476   26\n",
      "  400  317   46    7    4    2 1029   13  104   88    4  381   15  297\n",
      "   98   32    2   56   26  141    6  194    2   18    4  226   22   21\n",
      "  134  476   26  480    5  144   30    2   18   51   36   28  224   92\n",
      "   25  104    4  226   65   16   38 1334   88   12   16  283    5   16\n",
      "    2  113  103   32   15   16    2   19  178   32]\n",
      "Label: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"X-vector: \"+str(x_train[0]))\n",
    "print(\"Label: \"+str(y_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "When directly working with text, we need an embedding layer, where words are represented by dense vectors where a vector represents the projection of the word into a continuous vector space.\n",
    "Look at https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/ for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Epoch 1/15\n",
      "32/32 [==============================] - 8s 118ms/step - loss: 0.6908 - accuracy: 0.5260 - mae: 0.4987 - val_loss: 0.6838 - val_accuracy: 0.6130 - val_mae: 0.4950\n",
      "Epoch 2/15\n",
      "32/32 [==============================] - 3s 97ms/step - loss: 0.6151 - accuracy: 0.7390 - mae: 0.4472 - val_loss: 0.5956 - val_accuracy: 0.6720 - val_mae: 0.4248\n",
      "Epoch 3/15\n",
      "32/32 [==============================] - 3s 108ms/step - loss: 0.4687 - accuracy: 0.7890 - mae: 0.3388 - val_loss: 0.6050 - val_accuracy: 0.6680 - val_mae: 0.4037\n",
      "Epoch 4/15\n",
      "32/32 [==============================] - 3s 104ms/step - loss: 0.2919 - accuracy: 0.8780 - mae: 0.2050 - val_loss: 0.5708 - val_accuracy: 0.7150 - val_mae: 0.3284\n",
      "Epoch 5/15\n",
      "32/32 [==============================] - 3s 103ms/step - loss: 0.1483 - accuracy: 0.9540 - mae: 0.1063 - val_loss: 0.7591 - val_accuracy: 0.6890 - val_mae: 0.3270\n",
      "Epoch 6/15\n",
      "32/32 [==============================] - 3s 107ms/step - loss: 0.1486 - accuracy: 0.9480 - mae: 0.1106 - val_loss: 0.7738 - val_accuracy: 0.6920 - val_mae: 0.3228\n",
      "Epoch 7/15\n",
      "32/32 [==============================] - 3s 103ms/step - loss: 0.0510 - accuracy: 0.9870 - mae: 0.0399 - val_loss: 0.8316 - val_accuracy: 0.6710 - val_mae: 0.3521\n",
      "Epoch 8/15\n",
      "32/32 [==============================] - 3s 100ms/step - loss: 0.0498 - accuracy: 0.9930 - mae: 0.0409 - val_loss: 0.8855 - val_accuracy: 0.6970 - val_mae: 0.3090\n",
      "Epoch 9/15\n",
      "32/32 [==============================] - 3s 102ms/step - loss: 0.0125 - accuracy: 0.9990 - mae: 0.0113 - val_loss: 1.0059 - val_accuracy: 0.7150 - val_mae: 0.2925\n",
      "Epoch 10/15\n",
      "32/32 [==============================] - 3s 106ms/step - loss: 0.0044 - accuracy: 1.0000 - mae: 0.0042 - val_loss: 1.1379 - val_accuracy: 0.7010 - val_mae: 0.3001\n",
      "Epoch 11/15\n",
      "32/32 [==============================] - 3s 100ms/step - loss: 0.0019 - accuracy: 1.0000 - mae: 0.0019 - val_loss: 1.2127 - val_accuracy: 0.7160 - val_mae: 0.2865\n",
      "Epoch 12/15\n",
      "32/32 [==============================] - 3s 105ms/step - loss: 0.0017 - accuracy: 1.0000 - mae: 0.0016 - val_loss: 1.3412 - val_accuracy: 0.7150 - val_mae: 0.2865\n",
      "Epoch 13/15\n",
      "32/32 [==============================] - 3s 106ms/step - loss: 9.5763e-04 - accuracy: 1.0000 - mae: 9.4743e-04 - val_loss: 1.3278 - val_accuracy: 0.6970 - val_mae: 0.3035\n",
      "Epoch 14/15\n",
      "32/32 [==============================] - 3s 102ms/step - loss: 9.3329e-04 - accuracy: 1.0000 - mae: 9.2509e-04 - val_loss: 1.3975 - val_accuracy: 0.7130 - val_mae: 0.2890\n",
      "Epoch 15/15\n",
      "32/32 [==============================] - 3s 105ms/step - loss: 0.0016 - accuracy: 1.0000 - mae: 0.0016 - val_loss: 1.2414 - val_accuracy: 0.7080 - val_mae: 0.2893\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x268bed40d90>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "no_dim = 128\n",
    "\n",
    "# First we create an embedding for each word of dimensionality 128\n",
    "# no_dim - should match LSTM\n",
    "model.add(Embedding(max_features, no_dim))\n",
    "\n",
    "# dropout = percentage of units dropped by the input linear transformation\n",
    "# rec_drop = percentage of units dropped by linear transformation of recurrent state\n",
    "model.add(LSTM(no_dim, dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "# dimensionality of the output space = 1: since we use classification of a label, e.g., [0,1,2,3]\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy','mae'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation happens as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 15ms/step - loss: 1.2414 - accuracy: 0.7080 - mae: 0.2893\n",
      "{'loss': 1.2413833141326904, 'accuracy': 0.7080000042915344, 'mae': 0.2892940938472748}\n"
     ]
    }
   ],
   "source": [
    "evaluation = model.evaluate(x_test, y_test,return_dict = True)\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
